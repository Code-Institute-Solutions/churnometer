{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Fit and evaluate a cluster model to group similar customer behaviour\n",
        "* Understand profile for each cluster\n",
        "\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
        "* instructions on which variables to use for data cleaning and feature engineering. They are found on its respectives notebooks.\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/churnometer/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to make the parent of the parent of current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/churnometer'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKlJFX0iuM5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xk7DU_ekbtX8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7043, 17)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>SeniorCitizen</th>\n",
              "      <th>Partner</th>\n",
              "      <th>Dependents</th>\n",
              "      <th>PhoneService</th>\n",
              "      <th>MultipleLines</th>\n",
              "      <th>InternetService</th>\n",
              "      <th>OnlineSecurity</th>\n",
              "      <th>OnlineBackup</th>\n",
              "      <th>DeviceProtection</th>\n",
              "      <th>TechSupport</th>\n",
              "      <th>StreamingTV</th>\n",
              "      <th>StreamingMovies</th>\n",
              "      <th>Contract</th>\n",
              "      <th>PaperlessBilling</th>\n",
              "      <th>PaymentMethod</th>\n",
              "      <th>MonthlyCharges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No phone service</td>\n",
              "      <td>DSL</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Electronic check</td>\n",
              "      <td>29.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>One year</td>\n",
              "      <td>No</td>\n",
              "      <td>Mailed check</td>\n",
              "      <td>56.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>DSL</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Month-to-month</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Mailed check</td>\n",
              "      <td>53.85</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender  SeniorCitizen Partner Dependents PhoneService     MultipleLines  \\\n",
              "0  Female              0     Yes         No           No  No phone service   \n",
              "1    Male              0      No         No          Yes                No   \n",
              "2    Male              0      No         No          Yes                No   \n",
              "\n",
              "  InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport  \\\n",
              "0             DSL             No          Yes               No          No   \n",
              "1             DSL            Yes           No              Yes          No   \n",
              "2             DSL            Yes          Yes               No          No   \n",
              "\n",
              "  StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
              "0          No              No  Month-to-month              Yes   \n",
              "1          No              No        One year               No   \n",
              "2          No              No  Month-to-month              Yes   \n",
              "\n",
              "      PaymentMethod  MonthlyCharges  \n",
              "0  Electronic check           29.85  \n",
              "1      Mailed check           56.95  \n",
              "2      Mailed check           53.85  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = (pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\")\n",
        "      .drop(['customerID', 'TotalCharges', 'Churn', 'tenure' ],axis=1) \n",
        ")\n",
        "print(df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krjAk78Tbyhv"
      },
      "source": [
        "# Cluster Pipeline considering all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZWZHhpYaDjf"
      },
      "source": [
        "##  ML Cluster Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C6keis6ao8LA"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "### Feature Engineering\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "\n",
        "### Feat Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "### PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "### ML algorithm\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def PipelineCluster():\n",
        "  pipeline_base = Pipeline([\n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  variables = [ 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
        "                                                               'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                                               'OnlineBackup','DeviceProtection', 'TechSupport', \n",
        "                                                               'StreamingTV', 'StreamingMovies','Contract', \n",
        "                                                               'PaperlessBilling', 'PaymentMethod']) ),\n",
        "\n",
        "      (\"SmartCorrelatedSelection\",SmartCorrelatedSelection(variables=None, method=\"spearman\", \n",
        "                                                          threshold=0.6, selection_method=\"variance\") ),\n",
        "\n",
        "      (\"scaler\", StandardScaler()  ),  \n",
        "\n",
        "      (\"PCA\", PCA(n_components=50, random_state=0) ), \n",
        "\n",
        "      (\"model\", KMeans(n_clusters=50, random_state=0)    ), \n",
        "\n",
        "\n",
        "  ])\n",
        "  return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrr31sD9DyvY"
      },
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC9WunDaCuQo"
      },
      "source": [
        "We are interested to find the most suitable `n_components`, update the value on ML Pipeline for Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "es49S65qqvRw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7043, 12) \n",
            " <class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_pca = Pipeline(pipeline_cluster.steps[:-2])\n",
        "df_pca = pipeline_pca.fit_transform(df)\n",
        "\n",
        "print(df_pca.shape,'\\n', type(df_pca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlABEj9Iw6Jr"
      },
      "source": [
        "Apply PCA separately to the scaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cM_Xsqxsrt5M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* The 6 components explain 77.32% of the data \n",
            "\n",
            "             Explained Variance Ratio (%)\n",
            "Component 0                        23.947\n",
            "Component 1                        16.694\n",
            "Component 2                        12.093\n",
            "Component 3                         8.442\n",
            "Component 4                         8.281\n",
            "Component 5                         7.867\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "n_components = 6  # try 12 then 6\n",
        "pca = PCA(n_components=n_components).fit(df_pca)\n",
        "x_PCA = pca.transform(df_pca) # array with transformed PCA\n",
        "\n",
        "ComponentsList = [\"Component \" + str(number) for number in range(n_components)]\n",
        "dfExplVarRatio = pd.DataFrame(\n",
        "    data= np.round(100 * pca.explained_variance_ratio_ ,3),\n",
        "    index=ComponentsList,\n",
        "    columns=['Explained Variance Ratio (%)'])\n",
        "\n",
        "PercentageOfDataExplained = dfExplVarRatio['Explained Variance Ratio (%)'].sum()\n",
        "\n",
        "print(f\"* The {n_components} components explain {round(PercentageOfDataExplained,2)}% of the data \\n\")\n",
        "print(dfExplVarRatio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "  pipeline_base = Pipeline([\n",
        "      (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  variables = [ 'gender', 'Partner', 'Dependents', 'PhoneService',\n",
        "                                                               'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                                                               'OnlineBackup','DeviceProtection', 'TechSupport', \n",
        "                                                               'StreamingTV', 'StreamingMovies','Contract', \n",
        "                                                               'PaperlessBilling', 'PaymentMethod']) ),\n",
        "\n",
        "      (\"SmartCorrelatedSelection\",SmartCorrelatedSelection(variables=None, method=\"spearman\", \n",
        "                                                          threshold=0.6, selection_method=\"variance\") ),\n",
        "\n",
        "      (\"scaler\", StandardScaler()  ),  \n",
        "\n",
        "      (\"PCA\", PCA(n_components=6, random_state=0) ),  ### we update n_components to 6\n",
        "\n",
        "      (\"model\", KMeans(n_clusters=50, random_state=0)    ), \n",
        "\n",
        "\n",
        "  ])\n",
        "  return pipeline_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw9NtDj4EtEJ"
      },
      "source": [
        "## Elbow Method and Silhoutte Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txwWrETeDFS2"
      },
      "source": [
        "Find the most suitable `n_clusters`, update the value on ML Pipeline for Cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVaMnb9vGyBw"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1KLQujEvgi"
      },
      "source": [
        "Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZBcHjt7EwFT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(2,6), metric='silhouette')\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "for n_clusters in np.arange(start=2,stop=6):\n",
        "  \n",
        "  print(f\"=== Silhoutte plot for {n_clusters} Clusters ===\")\n",
        "  visualizer = SilhouetteVisualizer(estimator = KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                    colors = 'yellowbrick')\n",
        "  visualizer.fit(df_analysis)\n",
        "  visualizer.show()\n",
        "  plt.show()\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQBjAlRsHhU4"
      },
      "source": [
        "## Fit Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpxaylKk-6CQ"
      },
      "source": [
        "Quick recap in our data for training cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfKHc63v-6Zm"
      },
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfRpKC4Ykreg"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAiyUpTWHjQh"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L0iMkjJHXSI"
      },
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKT5IjmTmei8"
      },
      "source": [
        "We add a column \"`Clusters`\" (with the cluster pipeline predictions) to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ow8B0xVdmlgK"
      },
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAVrYJEqxYyG"
      },
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnjHhYjXng2r"
      },
      "source": [
        "Here we are saving the cluster predictions from this pipeline to use in a fututre moment. We will get back to that in a later stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWgb0kPOWtMa"
      },
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables = X['Clusters']\n",
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTWTf1rgkQ7b"
      },
      "source": [
        "## Fit a classifier, where target is cluster predictions and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6sGUn0XyDm"
      },
      "source": [
        "We copy `X` to a DataFrame `df_clf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeLq81sm2yAg"
      },
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b3Ei6Os5X3s"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgHXehCVyzUl"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZUk-uV5aN8"
      },
      "source": [
        "Create a classifier pipeline \n",
        "* We add a feature selection step that will tell the most relevant features to that pipeline\n",
        "* We are considering a model that typically offers good results and features importance can be assessed with `.features_importance_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Feat Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "### ML algorithm\n",
        "from sklearn.ensemble import GradientBoostingClassifier \n",
        "\n",
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningFeatEngFeatScaling()\n",
        "   pipe.steps.append([\"feat_selection\",SelectFromModel(GradientBoostingClassifier(random_state=0))])\n",
        "   pipe.steps.append([\"model\",GradientBoostingClassifier(random_state=0)])\n",
        "   return pipe\n",
        "  \n",
        "PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit the classifier to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R7xdg1Av0Ce"
      },
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z05LMFoZ4T2K"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1iqL2Kc544K"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Oo4xJMZ615p"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwjHBSh5ejG"
      },
      "source": [
        "## Assess Most Important Features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG5ztHxsKcd5"
      },
      "outputs": [],
      "source": [
        "# after data cleaning and feat engineering, the feature space changes\n",
        "\n",
        "data_cleaning_feat_eng_steps = 2 # how many data cleaning and feature engineering does your pipeline have?\n",
        "columns_after_data_cleaning_feat_eng = (Pipeline(pipeline_clf_cluster.steps[:data_cleaning_feat_eng_steps])\n",
        "                                        .transform(X_train)\n",
        "                                        .columns)\n",
        "\n",
        "best_features = columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()].to_list()\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "          'Feature': columns_after_data_cleaning_feat_eng[pipeline_clf_cluster['feat_selection'].get_support()],\n",
        "          'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        "  .sort_values(by='Importance', ascending=False)\n",
        "  )\n",
        "\n",
        "best_features = df_feature_importance['Feature'].to_list() # reassign best features in importance order\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{best_features} \\n\")\n",
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgul0EF9nx_E"
      },
      "source": [
        "We will store the best_features for future usage. We will get back to that in a later stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzyMkwHznyG8"
      },
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables = best_features\n",
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2ywCxJmkRQn"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMr-wiudEkb"
      },
      "source": [
        "Load function that plots a table with description for all Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lpRVDqTdEul"
      },
      "outputs": [],
      "source": [
        "def Clusters_IndividualDescription(EDA_Cluster,cluster):\n",
        "\n",
        "  ClustersDescription = pd.DataFrame(columns=EDA_Cluster.columns)\n",
        "  for col in EDA_Cluster.columns:\n",
        "    \n",
        "    try:  # eventually a given cluster will have only mssing data for a given variable\n",
        "      \n",
        "      if EDA_Cluster[col].dtypes == 'object':\n",
        "        \n",
        "        top_frequencies = EDA_Cluster.dropna(subset=[col])[[col]].value_counts(normalize=True).nlargest(n=3)\n",
        "        Description = ''\n",
        "        \n",
        "        for x in range(len(top_frequencies)):\n",
        "          freq = top_frequencies.iloc[x]\n",
        "          category = top_frequencies.index[x][0]\n",
        "          CategoryPercentage = int(round(freq*100,0))\n",
        "          statement =  f\"'{category}': {CategoryPercentage}% , \"  \n",
        "          Description = Description + statement\n",
        "        \n",
        "        ClustersDescription.at[0,col] = Description[:-2]\n",
        "\n",
        "\n",
        "      \n",
        "      elif EDA_Cluster[col].dtypes in ['float', 'int']:\n",
        "        DescStats = EDA_Cluster.dropna(subset=[col])[[col]].describe()\n",
        "        Q1 = int(round(DescStats.iloc[4,0],0))\n",
        "        Q3 = int(round(DescStats.iloc[6,0],0))\n",
        "        Description = f\"{Q1} -- {Q3}\"\n",
        "        ClustersDescription.at[0,col] = Description\n",
        "    \n",
        "    \n",
        "    except Exception as e:\n",
        "      ClustersDescription.at[0,col] = 'Not available'\n",
        "      print(f\"** Error Exception: {e} - cluster {cluster}, variable {col}\")\n",
        "  \n",
        "  ClustersDescription['Cluster'] = str(cluster)\n",
        "  \n",
        "  return ClustersDescription\n",
        "\n",
        "\n",
        "def DescriptionAllClusters(df_cluster_profile):\n",
        "\n",
        "  DescriptionAllClusters = pd.DataFrame(columns=df_cluster_profile.drop(['Clusters'],axis=1).columns)\n",
        "  for cluster in df_cluster_profile.sort_values(by='Clusters')['Clusters'].unique():\n",
        "    \n",
        "      EDA_ClusterSubset = df_cluster_profile.query(f\"Clusters == {cluster}\").drop(['Clusters'],axis=1)\n",
        "      ClusterDescription = Clusters_IndividualDescription(EDA_ClusterSubset,cluster)\n",
        "      DescriptionAllClusters = DescriptionAllClusters.append(ClusterDescription)\n",
        "\n",
        "  \n",
        "  DescriptionAllClusters.set_index(['Cluster'],inplace=True)\n",
        "  return DescriptionAllClusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHo7wmH68AYc"
      },
      "source": [
        "Load a custom function to plot cluster distribution per Variable (absolute and relative levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN23X2dT8AeA"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "def cluster_distribution_per_variable(df,target):\n",
        "\n",
        "\n",
        "  df_bar_plot = df.value_counts([\"Clusters\", target]).reset_index() \n",
        "  df_bar_plot.columns = ['Clusters',target,'Count']\n",
        "  df_bar_plot[target] = df_bar_plot[target].astype('object')\n",
        "\n",
        "  print(f\"Clusters distribution across {target} levels\")\n",
        "  fig = px.bar(df_bar_plot, x='Clusters',y='Count',color=target,width=800, height=500)\n",
        "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
        "  fig.show()\n",
        "\n",
        "\n",
        "  df_relative = (df\n",
        "                 .groupby([\"Clusters\", target])\n",
        "                 .size()\n",
        "                 .groupby(level=0)\n",
        "                 .apply(lambda x:  100*x / x.sum())\n",
        "                 .reset_index()\n",
        "                 .sort_values(by=['Clusters'])\n",
        "                 )\n",
        "  df_relative.columns = ['Clusters',target,'Relative Percentage (%)']\n",
        " \n",
        "\n",
        "  print(f\"Relative Percentage (%) of {target} in each cluster\")\n",
        "  fig = px.line(df_relative, x='Clusters',y='Relative Percentage (%)',color=target,width=800, height=500)\n",
        "  fig.update_layout(xaxis=dict(tickmode= 'array',tickvals= df['Clusters'].unique()))\n",
        "  fig.update_traces(mode='markers+lines')\n",
        "  fig.show()\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73J7J65v4O_d"
      },
      "source": [
        "Create a DataFrame that contains best features and Clusters Predictions, since we want to analyze the patterns for each cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PztdhjGl4Vkg"
      },
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "print(df_cluster_profile.shape)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mfJRrFc7wzu"
      },
      "source": [
        "We want also to analyze Churn levels\n",
        "* In this exercise, we get the data by loading from collection folder and filtering Churn.\n",
        "* We know in advance that Churn is encoded as an integer, even being an categorical variable. Therefore for this analysis we change its data type to `'object'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSRSNqiF4mnm"
      },
      "outputs": [],
      "source": [
        "df_churn = pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\").filter(['Churn'])\n",
        "df_churn['Churn'] = df_churn['Churn'].astype('object')\n",
        "df_churn.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD0Y3NdJOhm"
      },
      "source": [
        "### Cluster profile based on the best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDhycaSEdORm"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(pd.concat([df_cluster_profile,df_churn], axis=1))\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SS6CCCb74lH"
      },
      "source": [
        "### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwEUdPI2NHOb"
      },
      "outputs": [],
      "source": [
        "df_cluster_vs_churn=  df_churn.copy()\n",
        "df_cluster_vs_churn['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEfOvjx8ZhAH"
      },
      "source": [
        "# Fit New Cluster Pipeline only on most important features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFno4XnSZlZV"
      },
      "source": [
        "In order to reduce feature space, we will study the trade-off between the previous Cluster Pipeline (fitted with all variables) and Pipeline using the variables that are most important to define the clusters from the previous pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROH8cre2PHWx"
      },
      "outputs": [],
      "source": [
        "best_features_pipeline_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLOL2zr4Jr68"
      },
      "source": [
        "## Define trade-off and metrics to compare new and previous Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***** change tradeoff criteria"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJyxkSowZ9Cm"
      },
      "source": [
        "To evaluate this trade-off we will\n",
        "1. Conduct a elbow study and check if the same number of clusters is suggested\n",
        "2. Fit new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline\n",
        "3. Plot silhoutte score and compare to previous pipeline\n",
        "4. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar to the previous pipeline\n",
        "5. Check if the most important features for the classifier are the same from the previous pipeline\n",
        "6. Compare if the cluster profile from both pipelines are \"equivalent\"\n",
        "\n",
        "If we are happy to say **yes** for them, we can use a cluster pipeline using the features that best define the clusters from previous pipeline!\n",
        "* The **gain** is that in real time (which is the major purpose of Machine Learning) you will need less variables for predicting cluster for your prospects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxyTpm4EJx4s"
      },
      "source": [
        "## Subset data with the most relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQu9033oZ6r9"
      },
      "outputs": [],
      "source": [
        "df_reduced = df.filter(best_features_pipeline_all_variables)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ub9_YoIeaS5"
      },
      "source": [
        "## Rewrite ML pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrlQuieZeaS6"
      },
      "outputs": [],
      "source": [
        "def PipelineDataCleaningFeatEngFeatScaling():\n",
        "  pipeline_base = Pipeline([\n",
        "        # we update the pipeline, considering only the most important variables from previous pipeline      \n",
        "       (\"OrdinalCategoricalEncoder\",OrdinalEncoder(encoding_method='arbitrary',\n",
        "                                                  variables = ['OnlineBackup', 'PhoneService',\n",
        "                                                                # 'Partner',\t'Dependents'\n",
        "                                                                ] ) ),\n",
        "\n",
        "        # it doesn't need SmartCorrelation\n",
        "\n",
        "        (\"scaler\", StandardScaler()  ),\n",
        "\n",
        "    ])\n",
        "\n",
        "  return pipeline_base\n",
        "\n",
        "PipelineDataCleaningFeatEngFeatScaling()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def PipelineCluster():\n",
        "  pipe = PipelineDataCleaningFeatEngFeatScaling()\n",
        "  # No PCA step needed, since we know which features to consider\n",
        "  pipe.steps.append([\"model\",KMeans(n_clusters=3, random_state=0)])\n",
        "  return pipe\n",
        "\n",
        "PipelineCluster()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D57ncdQ7hBXe"
      },
      "source": [
        "## Apply Elbow Method and Silhoutte Score to the filtered data and compare to previous pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0wqQOM3hBXr"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_analysis = Pipeline(pipeline_cluster.steps[:-1])\n",
        "df_analysis = pipeline_analysis.fit_transform(df_reduced)\n",
        "\n",
        "print(df_analysis.shape,'\\n', type(df_analysis))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_1H05FKhBXs"
      },
      "source": [
        "Elbow Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsAJW4s0hBXt"
      },
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(1,11))\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "print(\"=== Average Silhouette Score for different number of clusters ===\")\n",
        "visualizer = KElbowVisualizer(KMeans(random_state=0), k=(2,6), metric='silhouette')\n",
        "visualizer.fit(df_analysis) \n",
        "visualizer.show() \n",
        "plt.show()\n",
        "print(\"\\n\")\n",
        "\n",
        "for n_clusters in np.arange(start=2,stop=6):\n",
        "  \n",
        "  print(f\"=== Silhoutte plot for {n_clusters} Clusters ===\")\n",
        "  visualizer = SilhouetteVisualizer(estimator = KMeans(n_clusters=n_clusters, random_state=0),\n",
        "                                    colors = 'yellowbrick')\n",
        "  visualizer.fit(df_analysis)\n",
        "  visualizer.show()\n",
        "  plt.show()\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_T1gtprhe8W"
      },
      "source": [
        "## Fit New Cluster Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEtzurhOhe8W"
      },
      "source": [
        "We set X as our training set for cluster. It is a copy of df_reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIZRR3wChe8X"
      },
      "outputs": [],
      "source": [
        "X = df_reduced.copy()\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-z3ST3nhe8Z"
      },
      "source": [
        "Fit Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAaEPy3Uhe8Z"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster = PipelineCluster()\n",
        "pipeline_cluster.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pdrt5bdKLDF"
      },
      "source": [
        "## Add cluster predictions to dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdFwZk1ihe8b"
      },
      "source": [
        "We add a column \"`Clusters`\" (with the cluster pipeline predictions) to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X['Clusters'] = pipeline_cluster['model'].labels_\n",
        "print(X.shape)\n",
        "X.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUwR8vCEhe8b"
      },
      "outputs": [],
      "source": [
        "print(f\"* Clusters frequencies \\n{ X['Clusters'].value_counts(normalize=True).to_frame().round(2)} \\n\\n\")\n",
        "X['Clusters'].value_counts().sort_values().plot(kind='bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUxjeMypKOUe"
      },
      "source": [
        "## Compare current cluster predictions to previous cluster predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6721kuGiII6"
      },
      "source": [
        "We just fitted a new cluster pipeline and want to compare if its predictions are \"equivalent\" from the previous cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAS2dDXQhe8c"
      },
      "source": [
        "These are the predictions from the **previous** cluster pipeline - trained with all variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbVaUAABhe8c"
      },
      "outputs": [],
      "source": [
        "cluster_predictions_with_all_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUiHzLIwimaD"
      },
      "source": [
        "And these are the predictions from **current** cluster pipeline (trained with `['OnlineBackup', 'MonthlyCharges', 'PhoneService', 'Partner', 'Dependents']`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kC7cKKCiwD5"
      },
      "outputs": [],
      "source": [
        "cluster_anwers_with_major_variables = X['Clusters'] \n",
        "cluster_anwers_with_major_variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jt-2n1GidBa"
      },
      "source": [
        "We use a confusion matrix to evaluate if the predictions of both pipelines are **\"equivalent\"**\n",
        "* We say equivalent in quotes, because we can't expect that a cluster label 0 in the previous cluster will have the same label in the current cluster. Eventually label 0 in previous cluster pipeline will be a different label in current cluster pipeline\n",
        "* When we reach this **equivalence**, it means both clusters \"clustered\" in a similar way but with different labels. And this is fine, since the label itself doesn't have meaning. We will look for the meaning after. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLy37N1TiiSx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(cluster_predictions_with_all_variables, cluster_anwers_with_major_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXrL1GUnjz1t"
      },
      "source": [
        "Conclusion\n",
        "* We see that both Clusters Pipelines are not predicting the data 100% equivalent, since few data points have different meaning for each pipeline. \n",
        "* However this is fine, and we say yes for this criteria. This is part of the trade-off we are up to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcPfalkwmomc"
      },
      "source": [
        "## Rewrite ML Pipeline for a classifier to explain the clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZClC5E5mome"
      },
      "source": [
        "We create a DataFrame `df_clf` as a copy of `X`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XoL6tuRmomf"
      },
      "outputs": [],
      "source": [
        "df_clf = X.copy()\n",
        "print(df_clf.shape)\n",
        "df_clf.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSfwpL-Fmomf"
      },
      "source": [
        "Split Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPyXs27Kmomf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test,y_train, y_test = train_test_split(\n",
        "                                    df_clf.drop(['Clusters'],axis=1),\n",
        "                                    df_clf['Clusters'],\n",
        "                                    test_size=0.2,\n",
        "                                    random_state=0\n",
        "                                    )\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W-cV2ts8A_N"
      },
      "source": [
        "Rewrite pipeline to explain clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm63GRYP8BIV"
      },
      "outputs": [],
      "source": [
        "def PipelineClf2ExplainClusters():\n",
        "   pipe = PipelineDataCleaningFeatEngFeatScaling()\n",
        "   # we don't consdier feature selection step, since we know which features to consider\n",
        "   pipe.steps.append([\"model\",GradientBoostingClassifier(random_state=0)])\n",
        "   return pipe\n",
        "\n",
        "PipelineClf2ExplainClusters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkaBhgjOmomg"
      },
      "source": [
        "## Fit a classifier, where target is cluster labels and features remaining variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU6mwsFYmomg"
      },
      "source": [
        "Create and fit a classifier pipeline to learn the feature importance when defining a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3liI7qjmomg"
      },
      "outputs": [],
      "source": [
        "pipeline_clf_cluster = PipelineClf2ExplainClusters()\n",
        "pipeline_clf_cluster.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCk6Swrmomh"
      },
      "source": [
        "## Evaluate classifier performance on Train and Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjXpF0x8momh"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_train, pipeline_clf_cluster.predict(X_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-Obn_Hcmomh"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, pipeline_clf_cluster.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-KlCS6MnBLk"
      },
      "source": [
        "The performance on Train and Test sets are similar, comparing to the previous pipeline! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G07251XWmomh"
      },
      "source": [
        "## Assess Most Important Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMTUNBYN8fyf"
      },
      "outputs": [],
      "source": [
        "best_features = X_train.columns.to_list() # since we don't have feature selection step in this pipeline, best_features is Xtrain columns\n",
        "\n",
        "# create DataFrame to display feature importance\n",
        "df_feature_importance = (pd.DataFrame(data={\n",
        "    'Feature':best_features,\n",
        "    'Importance': pipeline_clf_cluster['model'].feature_importances_})\n",
        ".sort_values(by='Importance', ascending=False)\n",
        ")\n",
        "\n",
        "best_features = df_feature_importance['Feature'].to_list()\n",
        "\n",
        "# Most important features statement and plot\n",
        "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
        "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
        "\n",
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that Partner increased its importance and Dependents decreased its importance. Online Backup decreased a bit its importance compared to the previous pipeline\n",
        "* We may expect difference since the training data was processed differently in both pipelines, in the first pipeline we had PCA step for example.\n",
        "* At the same the Feature Importance plot is clearly difference from boh pipelines. We will consider that as part of the trade-off analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q1TJSqdI6xK"
      },
      "source": [
        "## Cluster Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8EMIqE5I6xP"
      },
      "source": [
        "Create a DataFrame that contains best features and Clusters Predictions: we want to analyze the patterns for each cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEg92vdnI6xP"
      },
      "outputs": [],
      "source": [
        "df_cluster_profile = df_clf.copy()\n",
        "df_cluster_profile = df_cluster_profile.filter(items=best_features + ['Clusters'], axis=1)\n",
        "df_cluster_profile.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "789CeA0WI6xQ"
      },
      "source": [
        "We want also to analyze Churn levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx335aqtI6xR"
      },
      "outputs": [],
      "source": [
        "df_churn = pd.read_csv(\"outputs/datasets/collection/TelcoCustomerChurn.csv\").filter(['Churn'])\n",
        "df_churn['Churn'] = df_churn['Churn'].astype('object')\n",
        "df_churn.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-7jYzhtI6xR"
      },
      "source": [
        "### Cluster profile on most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urBmw5HJI6xS"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "clusters_profile = DescriptionAllClusters(pd.concat([df_cluster_profile,df_churn], axis=1))\n",
        "clusters_profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDJRuBBgI6xS"
      },
      "source": [
        "### Clusters distribution across Churn levels & Relative Percentage of Churn in each cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjMnAqYKI6xS"
      },
      "outputs": [],
      "source": [
        "df_cluster_vs_churn=  df_churn.copy()\n",
        "df_cluster_vs_churn['Clusters'] = X['Clusters']\n",
        "cluster_distribution_per_variable(df=df_cluster_vs_churn, target='Churn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xhPLbC4dwXL"
      },
      "source": [
        "## Which pipeline should I keep?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8qASh5k1jph"
      },
      "source": [
        "Let's recap the criteria we consider to evaluate the **trade-off**\n",
        "1. Conduct a elbow study and check if the same number of clusters is suggested\n",
        "2. Fit new cluster pipeline and compare if the predictions from this pipeline are \"equivalent\" to the predictions from the previous pipeline\n",
        "3. Plot silhoutte score and compare to previous pipeline\n",
        "4. Fit a classifier to explain cluster, and check if performance on Train and Test sets is similar to the previous pipeline\n",
        "5. Check if the most important features for the classifier are the same from the previous pipeline\n",
        "6. Compare if the cluster profile from both pipelines are \"equivalent\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPbS7JYN101K"
      },
      "source": [
        "We are happy with all criteria above for the new Cluster Pipeline, except for item 5; since the feature importance order and magnitude changed from one pipeline to another\n",
        "\n",
        "* Since we have 6 and 5 were positive, we will embrace the trade-off and will support the second pipeline. The benefits surpass the cost.\n",
        "* In addition, there is a great gain to have less variables for predicting live data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4HsuSuqd0g_"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zxpjktKd1n6"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i9X1oOORAQc"
      },
      "source": [
        "\n",
        "We will generate the following files\n",
        "\n",
        "* Cluster Pipeline\n",
        "* Train Set\n",
        "* Feature importance plot\n",
        "* Clusters Description\n",
        "* Cluster Silhouette\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ySBIrV1Q4cY"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "version = 'v1'\n",
        "file_path = f'outputs/ml_pipeline/cluster_analysis/{version}'\n",
        "\n",
        "try:\n",
        "  os.makedirs(name=file_path)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6y9-0fisd5cl"
      },
      "source": [
        "## Cluster pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfv9k5xMd7fv"
      },
      "outputs": [],
      "source": [
        "pipeline_cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsphnIR84hJ4"
      },
      "outputs": [],
      "source": [
        "joblib.dump(value=pipeline_cluster, filename=f\"{file_path}/cluster_pipeline.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ORnkwG6d74O"
      },
      "source": [
        "## Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqcwHaVwd9Ff"
      },
      "outputs": [],
      "source": [
        "print(df_reduced.shape)\n",
        "df_reduced.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M26MiJ9Y485Q"
      },
      "outputs": [],
      "source": [
        "df_reduced.to_csv(f\"{file_path}/TrainSet.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_lcQVXaV0p"
      },
      "source": [
        "## Most important features plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9datNfsLCVV"
      },
      "source": [
        "These are the features that define a cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYeoH7fjaV8J"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr0cVVQsaZqk"
      },
      "outputs": [],
      "source": [
        "df_feature_importance.plot(kind='bar',x='Feature',y='Importance', figsize=(8,4))\n",
        "plt.savefig(f\"{file_path}/features_define_cluster.png\", bbox_inches='tight', dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3Z5ivNd9mw"
      },
      "source": [
        "## Cluster Profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw-mEnI8d_Bv"
      },
      "outputs": [],
      "source": [
        "clusters_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G5CsAl738p7"
      },
      "outputs": [],
      "source": [
        "clusters_profile.to_csv(f\"{file_path}/clusters_profile.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RObeac1HQq5a"
      },
      "source": [
        "## Cluster silhouette plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualizer = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick')\n",
        "\n",
        "visualizer.fit(df_transformed)\n",
        "visualizer.show()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(7,5))\n",
        "fig = SilhouetteVisualizer(Pipeline(pipeline_cluster.steps[-1:])[0] , colors='yellowbrick', ax=axes)\n",
        "fig.fit(df_transformed)\n",
        "\n",
        "plt.savefig(f\"{file_path}/clusters_silhouette.png\", bbox_inches='tight',dpi=150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeIEUrWkJ-6_"
      },
      "source": [
        "Good job, clear the cell ouputs, run git commands to add, commit and push files to the repo. Next, we will move on to create our dashboard!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Modeling and Evaluation - Cluster Sklearn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
